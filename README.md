We investigated strategies to classify multi-label image data using CNNs with PyTorch, including using relative weighted losses in a [multi-label model](https://learnopencv.com/multi-label-image-classification-with-pytorch/), comparing the use of either [MobileNetV2](https://arxiv.org/abs/1801.04381) or [ResNet50](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf) as a backbone model; creating auxiliary tasks using pseudo-labelled data as in the SESEMI technique, for both previously labelled and unlabeled data; creating pseudo-labels; augmenting the labelled training data and adapting learning rates.

Our CNN architectures comprised a backbone and several heads: one for each label implementing multi-label classification and one for implementing SESEMI semi-supervised learning using a pseudo-labelled dataset. Training is done in three phases: first, with the unaugmented labelled training data; second, with the augmented labelled data; and third, with pseudo-labelled training data generated from the unlabeled training data. In later stages of training, the learning rate was manually decreased to not jump past optimal results.
